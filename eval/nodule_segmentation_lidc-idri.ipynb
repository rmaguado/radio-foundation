{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fb121a-b1df-4b5b-a34f-fd3ac544ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from functools import partial\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from dinov2.data.datasets import LidcIdri\n",
    "from dinov2.models import vision_transformer as vits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb1deb4-31bc-494c-ae97-f41a8c501ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_path = \"../runs/ctc_104x5x4/\"\n",
    "path_to_checkpoint = os.path.join(run_path, \"eval/training_399999/teacher_checkpoint.pth\")\n",
    "path_to_cfg = os.path.join(run_path, \"config.yaml\")\n",
    "cfg = OmegaConf.load(path_to_cfg)\n",
    "state_dict = torch.load(path_to_checkpoint, map_location=\"cpu\")[\"teacher\"]\n",
    "state_dict = {key: value for key, value in state_dict.items() if 'backbone' in key}\n",
    "state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n",
    "\n",
    "args = cfg.student\n",
    "img_size = cfg.augmentations.crops.global_crops_size\n",
    "\n",
    "vit_kwargs = dict(\n",
    "    img_size=img_size,\n",
    "    patch_size=args.patch_size,\n",
    "    in_chans=args.channels,\n",
    "    ffn_layer=args.ffn_layer,\n",
    "    qkv_bias=args.qkv_bias,\n",
    "    proj_bias=args.proj_bias,\n",
    "    ffn_bias=args.ffn_bias,\n",
    "    num_register_tokens=args.num_register_tokens,\n",
    "    interpolate_offset=args.interpolate_offset,\n",
    "    interpolate_antialias=args.interpolate_antialias,\n",
    "    \n",
    "    init_values=1,\n",
    "    block_chunks=0,\n",
    ")\n",
    "dino = vits.__dict__[args.arch](**vit_kwargs)\n",
    "dino.load_state_dict(state_dict, strict=True)\n",
    "dino.cuda()\n",
    "print(\"Loaded model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a421616-8ef0-44ad-a60a-b515167a01ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = \"../datasets/LIDC-IDRI/data\"\n",
    "dataset_extra = \"../datasets/LIDC-IDRI/extra\"\n",
    "train_dataset = LidcIdri(split=\"TRAIN\", root=dataset_root, extra=dataset_extra, enable_targets=True)\n",
    "val_dataset = LidcIdri(split=\"VAL\", root=dataset_root, extra=dataset_extra, enable_targets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bb1da9-ed43-44a1-a752-8ec6ad56204f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d613c76c-1182-4586-bb3f-dce3fcda5f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=0.124, std=0.121)\n",
    "\n",
    "def collate_fn(inputs):\n",
    "    batch = dict()\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in inputs:\n",
    "        image, label = i[0], i[1]\n",
    "        \n",
    "        image = image.resize((224, 224))\n",
    "        label = label.resize((224, 224))\n",
    "        \n",
    "        image = np.array(image).astype(np.float32)\n",
    "        label = np.array(label).astype(np.float32)\n",
    "        \n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        label = np.expand_dims(label, axis=0)\n",
    "        \n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "    \n",
    "    images = torch.stack([normalize(torch.from_numpy(img)) for img in images], dim=0)\n",
    "    labels = torch.stack([torch.from_numpy(lbl) for lbl in labels], dim=0)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=collate_fn, num_workers=10)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644b2aa1-06e7-4106-9cc6-3a0598da8fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(image, target):\n",
    "    np_image = np.array(image)\n",
    "    np_target = np.array(target)\n",
    "    \n",
    "    image_3 = np.repeat(np_image[:, :, np.newaxis], 3, axis=2)\n",
    "    target_3 = np.repeat(np_target[:, :, np.newaxis], 3, axis=2)\n",
    "    target_3[:,:,1:] = 0\n",
    "    \n",
    "    combined = np.clip(image_3 + target_3, 0, 1)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_figheight(15)\n",
    "    fig.set_figwidth(15)\n",
    "    ax1.imshow(image_3)\n",
    "    ax1.set_title(\"Image\")\n",
    "    ax2.imshow(combined)\n",
    "    ax2.set_title(\"Annotated\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03156a0a-7ea6-46b7-8be1-81a03261c4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, embed_dim, patch_size, num_labels=1):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_size = patch_size\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim,embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim,channels * patch_size**2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim,num_labels)\n",
    "        )\n",
    "        self.conv = nn.Conv2d(channels, num_labels, (1,1))\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        B, num_patches, _ = embeddings.shape\n",
    "        patch_dim = int(np.sqrt(num_patches))\n",
    "        assert patch_dim * patch_dim == num_patches, \"num_patches must be a perfect square\"\n",
    "        \n",
    "        predictions = self.mlp(embeddings)\n",
    "        predictions = predictions.view(B, patch_dim, patch_dim, self.num_labels)\n",
    "        predictions = predictions.permute(0, 3, 1, 2)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "class DinoSegmentator(nn.Module):\n",
    "    def __init__(self, dino, classifier):\n",
    "        super(DinoSegmentator, self).__init__()\n",
    "        \n",
    "        self.dino = dino\n",
    "        self.classifier = classifier\n",
    "        \n",
    "        for param in self.dino.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dino.forward_features(x)[\"x_norm_patchtokens\"]\n",
    "        \n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e8ff08-e425-4c8e-bc7b-c4b7b07e5103",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LinearClassifier(\n",
    "    channels=1,\n",
    "    embed_dim=384,\n",
    "    patch_size=14,\n",
    "    num_labels=1\n",
    ")\n",
    "model = DinoSegmentator(dino, classifier).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f452cfb-68c5-4eb4-8c84-c80dc1a1b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedBCEWithLogitsLossPatch(nn.Module):\n",
    "    def __init__(self, pos_weight, patch_size):\n",
    "        super(WeightedBCEWithLogitsLossPatch, self).__init__()\n",
    "        self.pos_weight = pos_weight\n",
    "        self.patch_size = patch_size\n",
    "        self.bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        B, C, H, W = targets.shape\n",
    "        \n",
    "        targets_patches = F.max_pool2d(targets.float(), kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        \n",
    "        loss = self.bce(logits, targets_patches)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d539b841-fbd6-4d43-8859-5f7033a46693",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weight = torch.tensor([1 / 0.00002807], dtype=torch.float32).cuda()\n",
    "criterion = WeightedBCEWithLogitsLossPatch(pos_weight=pos_weight, patch_size=14)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a36ee5-1d70-4c55-8c02-619db6e36a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou_patch(pred, target, threshold=0.5):\n",
    "    pred = (pred > threshold).float()\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "    \n",
    "    if union == 0:\n",
    "        return torch.tensor(1.0) if intersection == 0 else torch.tensor(0.0)\n",
    "    \n",
    "    iou = intersection / union\n",
    "    return iou\n",
    "\n",
    "def compute_dice_patch(pred, target, threshold=0.5):\n",
    "    pred = (pred > threshold).float()\n",
    "    intersection = (pred * target).sum()\n",
    "    total_sum = pred.sum() + target.sum()\n",
    "    \n",
    "    if total_sum == 0:\n",
    "        return torch.tensor(1.0) if intersection == 0 else torch.tensor(0.0)\n",
    "    \n",
    "    dice = (2. * intersection) / total_sum\n",
    "    return dice\n",
    "\n",
    "def compute_pixel_accuracy_patch(pred, target, threshold=0.5):\n",
    "    pred = (pred > threshold).float()\n",
    "    correct = (pred == target).sum()\n",
    "    total = target.numel()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c737219-5b58-4655-acdd-145539188e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_freq = 5\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Train\", unit=\"batch\")\n",
    "    for images, labels in train_loader_tqdm:\n",
    "        images_cu = images.cuda()\n",
    "        labels_cu = labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images_cu)\n",
    "        loss = criterion(outputs, labels_cu)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        train_loader_tqdm.set_postfix(loss=running_loss / (train_loader_tqdm.n + 1))\n",
    "    \n",
    "    if epoch % eval_freq == 0:\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_iou = 0\n",
    "        val_dice = 0\n",
    "        val_pixel_acc = 0\n",
    "        num_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_loader_tqdm = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Val\", unit=\"batch\")\n",
    "            for images, labels in val_loader_tqdm:\n",
    "                images_cu = images.cuda()\n",
    "                labels_cu = labels.cuda()\n",
    "\n",
    "                outputs = model(images_cu)\n",
    "                outputs = torch.sigmoid(outputs)\n",
    "\n",
    "                loss = criterion(outputs, labels_cu)\n",
    "                val_loss += loss.item()\n",
    "                val_iou += compute_iou(outputs, labels_cu).item()\n",
    "                val_dice += compute_dice(outputs, labels_cu).item()\n",
    "                val_pixel_acc += compute_pixel_accuracy(outputs, labels_cu).item()\n",
    "                num_samples += 1\n",
    "                val_loader_tqdm.set_postfix(val_loss=val_loss / (val_loader_tqdm.n + 1),\n",
    "                                            val_iou=val_iou / num_samples,\n",
    "                                            val_dice=val_dice / num_samples,\n",
    "                                            val_pixel_acc=val_pixel_acc / num_samples)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {running_loss / len(train_loader)}, '\n",
    "              f'Val Loss: {val_loss / len(val_loader)}, IoU: {val_iou / num_samples}, '\n",
    "              f'Dice: {val_dice / num_samples}, Pixel Accuracy: {val_pixel_acc / num_samples}')\n",
    "\n",
    "save_path = os.path.join(run_path, \"eval/training_99999/segmentation/classifier.pth\")\n",
    "torch.save(model.classifier.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0db340-9d1b-4e3a-8bb1-296c5ab928da",
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_positive_idx = None\n",
    "for i in range(len(val_dataset)):\n",
    "    img, tar = val_dataset[i]\n",
    "    if np.sum(np.array(tar).astype(np.float32)) > 0:\n",
    "        contains_positive_idx = i\n",
    "        break\n",
    "contains_positive_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818135bb-7bbd-4558-a636-0d4bdeada04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = val_dataset[167]\n",
    "        \n",
    "image = image.resize((224, 224))\n",
    "label = label.resize((224, 224))\n",
    "\n",
    "image = np.array(image).astype(np.float32)\n",
    "label = np.array(label).astype(np.float32)\n",
    "\n",
    "image = np.expand_dims(image, axis=0)\n",
    "label = np.expand_dims(label, axis=0)\n",
    "\n",
    "image = normalize(torch.from_numpy(image)).view((1,1,224,224))\n",
    "label = torch.from_numpy(label).view((1,1,224,224))\n",
    "\n",
    "output = model(image.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9fef86-14cd-4ecc-b94a-74ba12aacb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image[0][0], cmap=\"gray\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f224707-4fce-4d0b-aada-6a1b96381350",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(label[0][0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a71c75-a412-4b6c-b88c-a5e3693537d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = output.detach().cpu()[0][0]\n",
    "sample_output = torch.sigmoid(sample_output).numpy()\n",
    "plt.imshow(sample_output, vmin=0, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e1673a-8cc5-4510-a773-a55ad9c0977b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

import os
from pprint import pprint

from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from nltk.tokenize import word_tokenize
from rouge_score import rouge_scorer
import numpy as np
import pandas as pd
import argparse

from evaluate import load


class RadiologyReportEvaluator:
    """
    A class to evaluate generated radiology reports against reference reports
    using a variety of NLP metrics.
    """

    def __init__(self, reference_reports, generated_reports):
        """
        Initializes the evaluator with reference and generated reports.

        Args:
            reference_reports (list of str): The ground truth reports.
            generated_reports (list of str): The reports generated by the model.
        """
        if len(reference_reports) != len(generated_reports):
            raise ValueError(
                "Reference and generated report lists must have the same length."
            )

        self.reference_reports = reference_reports
        self.generated_reports = generated_reports
        self.results = {}

    def _preprocess_text(self, text):
        """Tokenizes text for evaluation."""
        return word_tokenize(text.lower())

    def evaluate_bleu(self):
        """
        Calculates the BLEU (Bilingual Evaluation Understudy) score.
        Measures how many n-grams (phrases) in the generated text appear in the reference.
        Good for measuring fluency and grammatical correctness.
        """
        bleu_scores = []
        chencherry = SmoothingFunction()
        for ref, gen in zip(self.reference_reports, self.generated_reports):
            ref_tokens = [self._preprocess_text(ref)]  # list of references
            gen_tokens = self._preprocess_text(gen)
            score = sentence_bleu(
                ref_tokens, gen_tokens, smoothing_function=chencherry.method1
            )
            bleu_scores.append(score)

        self.results["BLEU"] = np.mean(bleu_scores)

    def evaluate_rouge(self):
        """
        Calculates ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores.
        Measures the overlap of n-grams between the reference and generated text.
        Useful for assessing content recall.
        - ROUGE-1: Unigram overlap (individual words).
        - ROUGE-2: Bigram overlap (pairs of words).
        - ROUGE-L: Longest common subsequence.
        """
        scorer = rouge_scorer.RougeScorer(
            ["rouge1", "rouge2", "rougeL"], use_stemmer=True
        )
        rouge1_scores = []
        rouge2_scores = []
        rougeL_scores = []

        for ref, gen in zip(self.reference_reports, self.generated_reports):
            scores = scorer.score(ref, gen)
            rouge1_scores.append(scores["rouge1"].fmeasure)
            rouge2_scores.append(scores["rouge2"].fmeasure)
            rougeL_scores.append(scores["rougeL"].fmeasure)

        self.results["ROUGE-1"] = np.mean(rouge1_scores)
        self.results["ROUGE-2"] = np.mean(rouge2_scores)
        self.results["ROUGE-L"] = np.mean(rougeL_scores)

    def evaluate_bertscore(self):
        """
        Calculates BERTScore.
        Compares the contextual embeddings of tokens in the reference and generated texts.
        Generally considered more robust for semantic similarity than n-gram-based metrics.
        """
        bertscore = load("bertscore")
        # Ensure model_type is appropriate for medical text if possible
        results = bertscore.compute(
            predictions=self.generated_reports,
            references=self.reference_reports,
            lang="en",
            model_type="microsoft/deberta-xlarge-mnli",
            verbose=True,
        )
        self.results["BERTScore_Precision"] = np.mean(results["precision"])
        self.results["BERTScore_Recall"] = np.mean(results["recall"])
        self.results["BERTScore_F1"] = np.mean(results["f1"])

    def run_evaluation(self):
        """Runs all evaluation metrics."""
        print("Running evaluations...")
        self.evaluate_bleu()
        self.evaluate_rouge()
        self.evaluate_bertscore()
        print("Evaluation complete.")
        return self.results


def load_reports(run_name, combine=True):
    true_reports_path = (
        "mllm/preprocessing/out/abnormalities/valid/combined_reports.csv"
    )
    generated_reports_folder_path = f"runs/{run_name}/evaluation"

    true_reports_df = pd.read_csv(true_reports_path, sep=";")
    true_reports_df["base_volume_name"] = true_reports_df["VolumeName"].apply(
        lambda x: (
            "_".join(x.split("_")[:-1])
            if x.endswith(".nii.gz")
            and x.split("_")[-1].replace(".nii.gz", "").isdigit()
            else x.replace(".nii.gz", "")
        )
    )
    true_reports_df = true_reports_df.drop_duplicates(subset=["base_volume_name"])
    true_reports_df = true_reports_df.set_index("base_volume_name")

    generated_reports_map = {}

    filenames = os.listdir(generated_reports_folder_path)
    for file in filenames:
        base_name_with_ext = "_".join(file.replace(".txt", "").split("_")[:-1])
        base_name = base_name_with_ext

        generated_report_path = os.path.join(generated_reports_folder_path, file)

        with open(generated_report_path, "r") as f:
            report = f.read()

        if base_name not in generated_reports_map:
            generated_reports_map[base_name] = []
        generated_reports_map[base_name].append(report)

    combined_generated_reports = []
    corresponding_true_reports = []

    for base_name in sorted(generated_reports_map.keys()):
        if combine:
            combined_report = " ".join(generated_reports_map[base_name])
            combined_generated_reports.append(combined_report)

            true_report = true_reports_df.loc[base_name, "report"]
            corresponding_true_reports.append(true_report)

        else:
            true_report = true_reports_df.loc[base_name, "report"]
            for rep in generated_reports_map[base_name]:
                combined_generated_reports.append(rep)
                corresponding_true_reports.append(true_report)

    return corresponding_true_reports, combined_generated_reports


def main(run_name):

    true_reports, generated_reports = load_reports(run_name)

    evaluator = RadiologyReportEvaluator(
        reference_reports=true_reports,
        generated_reports=generated_reports,
    )

    evaluation_scores = evaluator.run_evaluation()

    print("\n--- Evaluation Results ---")
    pprint(evaluation_scores)
    print("--------------------------\n")

    print("Metric Explanations:")
    print("- BLEU: Measures n-gram precision (fluency).")
    print("- ROUGE-L: Measures longest common subsequence (content recall).")
    print("- BERTScore F1: Measures semantic similarity using contextual embeddings.")


def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--run_name", type=str)

    return parser.parse_args()


if __name__ == "__main__":
    args = get_args()
    run_name = args.run_name
    print(run_name)
    main(run_name)

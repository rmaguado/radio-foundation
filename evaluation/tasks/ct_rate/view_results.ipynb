{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from prettytable import PrettyTable\n",
    "import seaborn as sns\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "METRICS = [\"roc_auc\", \"f1\", \"precision\", \"recall\", \"specificity\", \"jw5\", \"jw6\", \"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "project_path = os.getenv(\"PROJECTPATH\")\n",
    "data_path = os.getenv(\"DATAPATH\")\n",
    "\n",
    "experiment_name = \"test_cardiomegaly_cls\"\n",
    "label = \"Cardiomegaly\"\n",
    "results_path = os.path.join(project_path, \"runs\", experiment_name, \"results\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(\n",
    "    result_path: str, valid_name: int, epoch: int\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Folds and epochs are 0-indexed.\n",
    "    \"\"\"\n",
    "    epoch_predictions_path = os.path.join(\n",
    "        result_path, valid_name, \"predictions\", f\"epoch_{epoch:02d}.csv\"\n",
    "    )\n",
    "    epoch_predictions = pd.read_csv(epoch_predictions_path)\n",
    "    logits = epoch_predictions[\"logits\"].values\n",
    "    labels = epoch_predictions[\"labels\"].values\n",
    "\n",
    "    return logits, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold(\n",
    "    logits: np.ndarray | List[np.ndarray], labels: np.ndarray | List[np.ndarray]\n",
    "):\n",
    "    if not isinstance(logits, np.ndarray):\n",
    "        logits = np.concatenate(logits)\n",
    "        labels = np.concatenate(labels)\n",
    "\n",
    "    probabilities = 1 / (1 + np.exp(-logits))\n",
    "\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    best_threshold = 0.5\n",
    "    best_youden = 0\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        predictions = (probabilities >= threshold).astype(int)\n",
    "\n",
    "        sensitivity = np.sum((predictions == 1) & (labels == 1)) / np.sum(labels == 1)\n",
    "        specificity = np.sum((predictions == 0) & (labels == 0)) / np.sum(labels == 0)\n",
    "        youden = sensitivity + specificity - 1\n",
    "\n",
    "        if youden > best_youden:\n",
    "            best_youden = youden\n",
    "            best_threshold = threshold\n",
    "\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(\n",
    "    logits: np.ndarray, labels: np.ndarray, threshold=None\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate performance metrics based on logits and true labels.\n",
    "    Also returns the threshold determined using the youden index.\n",
    "    \"\"\"\n",
    "    roc_auc = roc_auc_score(labels, logits)\n",
    "\n",
    "    if threshold is None:\n",
    "        threshold = get_threshold(logits, labels)\n",
    "\n",
    "    probabilities = 1 / (1 + np.exp(-logits))\n",
    "\n",
    "    final_predictions = (probabilities >= threshold).astype(int)\n",
    "\n",
    "    final_precision = np.sum((final_predictions == 1) & (labels == 1)) / np.sum(\n",
    "        final_predictions == 1\n",
    "    )\n",
    "    final_recall = np.sum((final_predictions == 1) & (labels == 1)) / np.sum(\n",
    "        labels == 1\n",
    "    )\n",
    "    specificity = np.sum((final_predictions == 0) & (labels == 0)) / np.sum(labels == 0)\n",
    "    jw5 = (final_precision + final_recall) / 2\n",
    "    jw6 = final_precision * 0.4 + final_recall * 0.6\n",
    "    best_f1 = 2 * final_precision * final_recall / (final_precision + final_recall)\n",
    "\n",
    "    loss = -np.mean(labels * logits + (1 - labels) * (-logits))\n",
    "\n",
    "    return {\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"f1\": best_f1,\n",
    "        \"precision\": final_precision,\n",
    "        \"recall\": final_recall,\n",
    "        \"specificity\": specificity,\n",
    "        \"jw5\": jw5,\n",
    "        \"jw6\": jw6,\n",
    "        \"loss\": loss,\n",
    "        \"threshold\": threshold,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_results(\n",
    "    results_path: str, n_folds: int = 5, n_epochs: int = 10, default_threshold=None\n",
    ") -> Tuple[Dict[str, List[float]], Dict[str, List[Tuple[float, float]]]]:\n",
    "    \"\"\"\n",
    "    Parse cross-validation results. Returns metrics for each epoch and fold, along with confidence intervals.\n",
    "\n",
    "    Args:\n",
    "        results_path: Path to results directory.\n",
    "        n_folds: Number of folds.\n",
    "        n_epochs: Number of epochs.\n",
    "\n",
    "    Returns:\n",
    "        metrics_epoch_fold: Dictionary of metrics for each epoch and fold.\n",
    "        confidence_intervals: Dictionary of mean and standard deviation for each metric and each epoch.\n",
    "    \"\"\"\n",
    "    epochs_performance = {\n",
    "        m: {ep: [] for ep in range(n_epochs)} for m in METRICS + [\"threshold\"]\n",
    "    }\n",
    "\n",
    "    for ep in range(n_epochs):\n",
    "        logits, labels = zip(\n",
    "            *[get_logits(results_path, f\"fold_{i}\", ep) for i in range(n_folds)]\n",
    "        )\n",
    "\n",
    "        if default_threshold is None:\n",
    "            threshold = get_threshold(logits, labels)\n",
    "        else:\n",
    "            threshold = default_threshold\n",
    "\n",
    "        for i in range(n_folds):\n",
    "\n",
    "            performance_metrics = get_metrics(logits[i], labels[i], threshold)\n",
    "            for metric_name, metric_value in performance_metrics.items():\n",
    "                epochs_performance[metric_name][ep].append(metric_value)\n",
    "\n",
    "    confidence_intervals = {\n",
    "        m: [(np.mean(perf), np.std(perf)) for perf in epochs_performance[m].values()]\n",
    "        for m in METRICS\n",
    "    }\n",
    "\n",
    "    return epochs_performance, confidence_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_results(\n",
    "    results_path: str, n_epochs: int = 10, default_threshold=None\n",
    ") -> Tuple[Dict[str, List[float]], Dict[str, List[Tuple[float, float]]]]:\n",
    "\n",
    "    epochs_performance = {\n",
    "        m: {ep: None for ep in range(n_epochs)} for m in METRICS + [\"threshold\"]\n",
    "    }\n",
    "\n",
    "    for ep in range(n_epochs):\n",
    "        logits, labels = get_logits(results_path, \"test\", ep)\n",
    "\n",
    "        if default_threshold is None:\n",
    "            threshold = get_threshold(logits, labels)\n",
    "        else:\n",
    "            threshold = default_threshold\n",
    "\n",
    "        performance_metrics = get_metrics(logits, labels, threshold)\n",
    "        for metric_name, metric_value in performance_metrics.items():\n",
    "            epochs_performance[metric_name][ep] = metric_value\n",
    "\n",
    "    return epochs_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_epoch_from_metric(\n",
    "    epochs_performance: Dict[str, List[float]], selection_metric: str = \"roc_auc\"\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Select the best epoch based on the specified metric.\n",
    "    \"\"\"\n",
    "    epoch_mean_selection = [\n",
    "        np.mean(perf) for perf in epochs_performance[selection_metric].values()\n",
    "    ]\n",
    "\n",
    "    if \"loss\" in selection_metric:\n",
    "        best_epoch = np.argmin(epoch_mean_selection)\n",
    "    else:\n",
    "        best_epoch = np.argmax(epoch_mean_selection)\n",
    "\n",
    "    return int(best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simple_folds_plot(\n",
    "    cv_performance: Dict[str, List[float]],\n",
    "    label: str,\n",
    "    metric: str,\n",
    "    num_folds=5,\n",
    "    num_epochs=10,\n",
    "    cm=plt.cm.tab10,\n",
    ") -> None:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "\n",
    "    for fold_idx in range(num_folds):\n",
    "        epoch_metrics = [cv_performance[metric][x][fold_idx] for x in range(num_epochs)]\n",
    "        plt.plot(\n",
    "            epochs,\n",
    "            epoch_metrics,\n",
    "            color=cm(fold_idx / num_folds),\n",
    "            label=f\"Fold {fold_idx + 1}\",\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend(\n",
    "        loc=\"lower center\",\n",
    "        ncol=num_folds,\n",
    "        bbox_to_anchor=(0.5, -0.25),\n",
    "        frameon=False,\n",
    "    )\n",
    "    plt.xticks(epochs)\n",
    "    plt.title(f\"Training Curve {metric} for {label}\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_folds_plot_all_metrics(\n",
    "    cv_performance: Dict[str, List[float]],\n",
    "    best_epoch_by_metric: Dict[str, int],\n",
    "    label: str,\n",
    "    num_folds=5,\n",
    "    num_epochs=10,\n",
    "    cm=plt.cm.tab10,\n",
    ") -> None:\n",
    "    n_metrics = len(METRICS)\n",
    "    fig_height = 3 * n_metrics\n",
    "    fig, axs = plt.subplots(n_metrics, figsize=(12, fig_height), sharex=True)\n",
    "\n",
    "    if n_metrics == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "\n",
    "    for metric_idx, metric in enumerate(METRICS):\n",
    "        best_epoch = best_epoch_by_metric[metric] + 1\n",
    "\n",
    "        ax = axs[metric_idx]\n",
    "        for fold_idx in range(num_folds):\n",
    "            epoch_metrics = [\n",
    "                cv_performance[metric][x][fold_idx] for x in range(num_epochs)\n",
    "            ]\n",
    "            ax.plot(\n",
    "                epochs,\n",
    "                epoch_metrics,\n",
    "                color=cm(fold_idx / num_folds),\n",
    "                label=f\"Fold {fold_idx + 1}\",\n",
    "                linewidth=2,\n",
    "                marker=\"o\",\n",
    "                markersize=4,\n",
    "            )\n",
    "\n",
    "        ax.axvline(\n",
    "            x=best_epoch,\n",
    "            color=\"black\",\n",
    "            linestyle=\"--\",\n",
    "            alpha=0.5,\n",
    "            linewidth=2,\n",
    "            label=\"Best Epoch\",\n",
    "        )\n",
    "\n",
    "        ax.set_xticks(epochs)\n",
    "        ax.set_ylabel(metric.upper())\n",
    "        ax.set_title(f\"{metric.upper()} Training Curve\", fontsize=12)\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    fig.legend(\n",
    "        handles,\n",
    "        labels,\n",
    "        loc=\"lower center\",\n",
    "        ncol=len(labels),\n",
    "        bbox_to_anchor=(0.5, 0.0),\n",
    "        frameon=False,\n",
    "    )\n",
    "\n",
    "    axs[-1].set_xlabel(\"Epoch\", fontsize=12)\n",
    "    fig.suptitle(\n",
    "        f\"{num_folds}-Fold Validation Performance for {label}\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 0.97])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ci_plot_all_metrics(\n",
    "    confidence_intervals: Dict[str, List[Tuple[float, float]]],\n",
    "    best_epoch_by_metric: Dict[str, int],\n",
    "    label: str,\n",
    "    num_epochs=10,\n",
    "    z_score=1.96,\n",
    "    ci_label: str = \"95% CI\",\n",
    "    full_y: bool = False,\n",
    ") -> None:\n",
    "    n_metrics = len(METRICS)\n",
    "    fig_height = 3 * n_metrics\n",
    "    fig, axs = plt.subplots(n_metrics, figsize=(12, fig_height), sharex=True)\n",
    "\n",
    "    if n_metrics == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "\n",
    "    for metric_idx, metric in enumerate(METRICS):\n",
    "        best_epoch = best_epoch_by_metric[metric] + 1\n",
    "\n",
    "        ax = axs[metric_idx]\n",
    "        means = [ci[0] for ci in confidence_intervals[metric]]\n",
    "        stds = [ci[1] for ci in confidence_intervals[metric]]\n",
    "        ci = [std * z_score for std in stds]\n",
    "\n",
    "        ax.fill_between(\n",
    "            epochs,\n",
    "            np.array(means) - np.array(ci),\n",
    "            np.array(means) + np.array(ci),\n",
    "            alpha=0.2,\n",
    "            color=\"navy\",\n",
    "            label=ci_label,\n",
    "        )\n",
    "\n",
    "        ax.plot(\n",
    "            epochs,\n",
    "            means,\n",
    "            color=\"navy\",\n",
    "            linewidth=2,\n",
    "            marker=\"o\",\n",
    "            markersize=4,\n",
    "            label=\"Mean\",\n",
    "            alpha=0.8,\n",
    "        )\n",
    "\n",
    "        ax.axvline(\n",
    "            x=best_epoch,\n",
    "            color=\"black\",\n",
    "            linestyle=\"--\",\n",
    "            alpha=0.5,\n",
    "            linewidth=2,\n",
    "            label=\"Best Epoch\",\n",
    "        )\n",
    "\n",
    "        ax.set_xticks(epochs)\n",
    "        ax.set_ylabel(metric.upper())\n",
    "        ax.set_title(f\"{metric.upper()} Confidence Intervals\", fontsize=12)\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    fig.legend(\n",
    "        handles,\n",
    "        labels,\n",
    "        loc=\"lower center\",\n",
    "        ncol=len(labels),\n",
    "        bbox_to_anchor=(0.5, 0.0),\n",
    "        frameon=False,\n",
    "    )\n",
    "\n",
    "    axs[-1].set_xlabel(\"Epoch\", fontsize=12)\n",
    "    fig.suptitle(\n",
    "        f\"Cross Validation Confidence Intervals for {label}\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 0.97])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_plot_all_metrics(\n",
    "    test_performance: Dict[str, List[float]],\n",
    "    label: str,\n",
    "    num_folds=5,\n",
    "    num_epochs=10,\n",
    ") -> None:\n",
    "    n_metrics = len(METRICS)\n",
    "    fig_height = 3 * n_metrics\n",
    "    fig, axs = plt.subplots(n_metrics, figsize=(12, fig_height), sharex=True)\n",
    "\n",
    "    if n_metrics == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "\n",
    "    for metric_idx, metric in enumerate(METRICS):\n",
    "        ax = axs[metric_idx]\n",
    "        epoch_metrics = [test_performance[metric][x] for x in range(num_epochs)]\n",
    "        ax.plot(\n",
    "            epochs,\n",
    "            epoch_metrics,\n",
    "            color=\"navy\",\n",
    "            label=f\"Test\",\n",
    "            linewidth=2,\n",
    "            marker=\"o\",\n",
    "            markersize=4,\n",
    "        )\n",
    "\n",
    "        ax.set_xticks(epochs)\n",
    "        ax.set_ylabel(metric.upper())\n",
    "        ax.set_title(f\"{metric.upper()} Training Curve\", fontsize=12)\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    fig.legend(\n",
    "        handles,\n",
    "        labels,\n",
    "        loc=\"lower center\",\n",
    "        ncol=len(labels),\n",
    "        bbox_to_anchor=(0.5, 0.0),\n",
    "        frameon=False,\n",
    "    )\n",
    "\n",
    "    axs[-1].set_xlabel(\"Epoch\", fontsize=12)\n",
    "    fig.suptitle(f\"Test Performance for {label}\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 0.97])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_density_plot(results_path, valid_name, epoch, label):\n",
    "\n",
    "    logits, labels = get_logits(results_path, valid_name, epoch)\n",
    "\n",
    "    prob_pos = logits[labels == 1]\n",
    "    prob_neg = logits[labels == 0]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    sns.kdeplot(prob_neg, label=\"Negative\", fill=True, alpha=0.5, color=\"tab:blue\")\n",
    "    sns.kdeplot(prob_pos, label=\"Positive\", fill=True, alpha=0.5, color=\"tab:orange\")\n",
    "\n",
    "    plt.xlabel(\"Predicted Probability\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.title(f\"Density of Predicted Probabilities for {label}\")\n",
    "    plt.legend(\n",
    "        loc=\"lower center\",\n",
    "        ncol=2,\n",
    "        bbox_to_anchor=(0.5, -0.25),\n",
    "        frameon=False,\n",
    "    )\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    plt.tight_layout(rect=[0, 0.07, 1, 0.97])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_density_plot(results_path, \"test\", 0, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_threshold = 0.5\n",
    "\n",
    "cv_performance, confidence_intervals = get_cv_results(\n",
    "    results_path, default_threshold=default_threshold\n",
    ")\n",
    "test_performance = get_test_results(results_path, default_threshold=default_threshold)\n",
    "\n",
    "best_epoch_by_metric = {\n",
    "    m: best_epoch_from_metric(cv_performance, selection_metric=m) for m in METRICS\n",
    "}\n",
    "\n",
    "table = PrettyTable([\"Metric\", \"Best Epoch\"])\n",
    "\n",
    "for key, value in best_epoch_by_metric.items():\n",
    "    table.add_row([key, value])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = PrettyTable([\"Epoch\", \"Best Threshold\"])\n",
    "\n",
    "for key, value in cv_performance[\"threshold\"].items():\n",
    "    table.add_row([key, f\"{value[0]:.2f}\"])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_folds_plot_all_metrics(cv_performance, best_epoch_by_metric, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_ci_plot_all_metrics(confidence_intervals, best_epoch_by_metric, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_test_plot_all_metrics(test_performance, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dino2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

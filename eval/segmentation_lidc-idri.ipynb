{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fb121a-b1df-4b5b-a34f-fd3ac544ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from collections import deque\n",
    "\n",
    "from eval.utils import (\n",
    "    load_model,\n",
    "    binary_accuracy_logits,\n",
    "    ImageTargetTransform,\n",
    "    LinearClassifier,\n",
    ")\n",
    "\n",
    "from eval.segmentation import sample_from_queues, process_batch, update_difficult_negatives\n",
    "from eval.extended_datasets import LidcIdriNodules, get_lidcidri_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7535502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "project_path = os.getenv(\"PROJECTPATH\")\n",
    "data_path = os.getenv(\"DATAPATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb1deb4-31bc-494c-ae97-f41a8c501ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_run = \"runs/base_103x4x5\"\n",
    "checkpoint_name = \"training_69999\"\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "feature_model, config = load_model(path_to_run, checkpoint_name, device)\n",
    "print(\"Loaded model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ab141c-16a7-4b1a-8fcb-4c0a28469fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_image_size = config.student.full_image_size\n",
    "patch_size = config.student.patch_size\n",
    "data_mean = -573.8\n",
    "data_std = 461.3\n",
    "channels = 4\n",
    "\n",
    "print(\"Full image size:\", full_image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a421616-8ef0-44ad-a60a-b515167a01ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_processor = ImageTargetTransform(full_image_size, data_mean, data_std)\n",
    "\n",
    "dataset_kwargs = {\n",
    "    \"root_path\": os.path.join(data_path, \"dicoms\"),\n",
    "    \"mask_path\": os.path.join(project_path, \"data/eval/LIDC-IDRI/masks\"),\n",
    "    \"transform\": img_processor\n",
    "}\n",
    "\n",
    "train_dataset = LidcIdriNodules(**dataset_kwargs)\n",
    "train_dataloader = get_lidcidri_loader(train_dataset, channels=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccb345f-8bc1-41c6-b959-1cc0a230525b",
   "metadata": {},
   "source": [
    "test_images, test_targets = next(train_dataloader)\n",
    "unit_batch = test_images[0].view(1, channels, full_image_size, full_image_size)\n",
    "with torch.no_grad():\n",
    "    outputs = feature_model(unit_batch.to(device))\n",
    "_, _, embed_dim = outputs[0][0].shape\n",
    "print(\"Embedding dimension:\", embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3070ef-3eaf-4c39-8b64-da8c8b9443ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce1b82-1fd3-4005-a462-eb8d9a415c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = embed_dim * 4\n",
    "PATCH_SIZE = config.student.patch_size\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "classifier_model = LinearClassifier(\n",
    "    embed_dim=EMBED_DIM, hidden_dim=2048, num_labels=1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03156a0a-7ea6-46b7-8be1-81a03261c4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_interval = 1_000\n",
    "max_iter = 2_000\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(classifier_model.parameters(), momentum=0.9, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, max_iter, eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e8ff08-e425-4c8e-bc7b-c4b7b07e5103",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_max_size = 2_000\n",
    "alpha = 0.95\n",
    "patch_embedding_cache = deque(maxlen=cache_max_size)\n",
    "negative_patch_queue = deque(maxlen=cache_max_size)\n",
    "\n",
    "iteration = 0\n",
    "while iteration < max_iter:\n",
    "    classifier_model.train()\n",
    "    running_loss = 0.0\n",
    "    train_tqdm = tqdm(range(1, eval_interval + 1), desc=f\"Training\", leave=False)\n",
    "\n",
    "    for i in train_tqdm:\n",
    "        inputs, targets = next(train_dataloader)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        patch_tokens, patch_labels = process_batch(\n",
    "            feature_model, inputs, targets, embed_dim, patch_size, device\n",
    "        )\n",
    "\n",
    "        masked_patch_indices = (patch_labels > 0).nonzero(as_tuple=True)\n",
    "        masked_patch_tokens = patch_tokens[masked_patch_indices]\n",
    "\n",
    "        patch_embedding_cache.extend(\n",
    "            patch.detach().cpu() for patch in masked_patch_tokens\n",
    "        )\n",
    "\n",
    "        negative_patch_indices = (patch_labels == 0).nonzero(as_tuple=True)\n",
    "        negative_patch_tokens = patch_tokens[negative_patch_indices]\n",
    "\n",
    "        negative_patch_queue.extend(\n",
    "            patch.detach().cpu() for patch in negative_patch_tokens\n",
    "        )\n",
    "\n",
    "        if len(patch_embedding_cache) == 0:\n",
    "            continue\n",
    "            \n",
    "        num_resample = min(\n",
    "            len(patch_embedding_cache)*2,\n",
    "            BATCH_SIZE\n",
    "        )\n",
    "        resampled_tokens, resampled_labels, neg_indices_from_queue = (\n",
    "            sample_from_queues(\n",
    "                patch_embedding_cache,\n",
    "                negative_patch_queue,\n",
    "                patch_tokens,\n",
    "                patch_labels,\n",
    "                num_resample,\n",
    "                device,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        classifier_output = classifier_model(resampled_tokens)\n",
    "        loss = criterion(classifier_output, resampled_labels)\n",
    "\n",
    "        update_difficult_negatives(\n",
    "            classifier_output,\n",
    "            resampled_labels,\n",
    "            resampled_tokens,\n",
    "            negative_patch_queue,\n",
    "            neg_indices_from_queue,\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(classifier_model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss = running_loss * alpha + (1 - alpha) * loss.item()\n",
    "        iteration += 1\n",
    "\n",
    "        train_tqdm.set_postfix(\n",
    "            {\"Loss\": running_loss, \"Cache\": len(patch_embedding_cache)}\n",
    "        )\n",
    "\n",
    "    classifier_model.eval()\n",
    "    accuracy_sum = 0.0\n",
    "    positives = 0\n",
    "    negatives = 0\n",
    "    true_pred_positives = 0\n",
    "    true_pred_negatives = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(val_dataloader, desc=f\"Evaluation\", leave=False):\n",
    "            features = feature_model(inputs.to(device))\n",
    "            patch_tokens = torch.cat(\n",
    "                [patch_token for patch_token, _ in features], dim=-1\n",
    "            ).view(-1, EMBED_DIM)\n",
    "            patch_labels = binary_mask_to_patch_labels(\n",
    "                targets.to(device), PATCH_SIZE\n",
    "            ).view(-1, 1)\n",
    "\n",
    "            classifier_output = classifier_model(patch_tokens)\n",
    "\n",
    "            accuracy, acc_breakdown = binary_accuracy_logits(\n",
    "                classifier_output, patch_labels\n",
    "            )\n",
    "            accuracy_sum += accuracy\n",
    "            positives += acc_breakdown[0]\n",
    "            true_pred_positives += acc_breakdown[1]\n",
    "            negatives += acc_breakdown[2]\n",
    "            true_pred_negatives += acc_breakdown[3]\n",
    "\n",
    "    avg_accuracy = accuracy_sum / len(val_dataloader)\n",
    "    avg_p_accuracy = true_pred_positives / positives\n",
    "    avg_n_accuracy = true_pred_negatives / negatives\n",
    "\n",
    "    print(\n",
    "        f\"Iteration: {iteration}, Overall Accuracy: {avg_accuracy:.4f}, Positives: {avg_p_accuracy}, Negatives: {avg_n_accuracy}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e7433b-bebc-4606-8215-34af4b8c4b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize some image segmentations\n",
    "demoiter = iter(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ef0627-1525-4df9-a15e-4897cdbe6e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(demoiter)\n",
    "patch_labels = binary_mask_to_patch_labels(targets.to(device), PATCH_SIZE)\n",
    "masked_patch_indices = (patch_labels > 0).nonzero(as_tuple=True)\n",
    "masked_patch_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e2e2bf-bcb3-4185-a8c1-26e997186d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = feature_model(images.to(device))\n",
    "patch_tokens = torch.cat([patch_token for patch_token, _ in features], dim=-1)\n",
    "classifier_output = classifier_model(patch_tokens)\n",
    "classifier_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6e8b49-8b83-4d29-93c9-f2b00a31c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_batch = 19\n",
    "\n",
    "original_image = images[selected_batch][0].numpy()\n",
    "true_mask = patch_labels[selected_batch].view(16, 16).cpu().numpy()\n",
    "predicted_mask = classifier_output[selected_batch].detach().view(16, 16).cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(original_image, cmap=\"gray\")\n",
    "axes[0].set_title(\"Image\")\n",
    "\n",
    "axes[1].imshow(true_mask, cmap=\"gray\")\n",
    "axes[1].set_title(\"True mask\")\n",
    "\n",
    "axes[2].imshow(predicted_mask, cmap=\"gray\")\n",
    "axes[2].set_title(\"Predicted mask\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3437c961-93ff-40a8-a1e9-c2e5819e5635",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(predicted_mask > 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e9cf09-dd80-4c40-a2bb-f306c645112a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dino2t24",
   "language": "python",
   "name": "dino2t24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

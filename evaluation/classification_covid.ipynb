{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fb121a-b1df-4b5b-a34f-fd3ac544ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from evaluation.utils.networks import FullScanPredictor\n",
    "from evaluation.extended_datasets.deeprdt_lung import DeepRDT_lung\n",
    "from evaluation.extended_datasets.covid_19_ny_sbu import COVID_19_NY_SBU\n",
    "from evaluation.utils.dataset import BalancedSampler, split_dataset, CombinedDataset\n",
    "from evaluation.utils.metrics import compute_metrics\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7535502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "project_path = os.getenv(\"PROJECTPATH\")\n",
    "data_path = os.getenv(\"DATAPATH\")\n",
    "\n",
    "run_name = \"base4_103x4x5\"\n",
    "checkpoint_name = \"training_659999\"\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a421616-8ef0-44ad-a60a-b515167a01ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = os.path.join(data_path, \"dicoms/DeepRDT-lung/metadata_lung_oldPat.csv\")\n",
    "dataset_control = DeepRDT_lung(metadata_path, run_name, checkpoint_name, label=\"False\")\n",
    "\n",
    "dataset_positive = COVID_19_NY_SBU(run_name, checkpoint_name)\n",
    "\n",
    "combined = CombinedDataset(dataset_control, dataset_positive)\n",
    "train_dataset, val_dataset = split_dataset(combined, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb52e6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    embeddings, labels = zip(*batch)\n",
    "\n",
    "    _, num_tokens, embed_dim = embeddings[0].shape\n",
    "\n",
    "    max_length = max([embedding.shape[0] for embedding in embeddings])\n",
    "\n",
    "    padded_embeddings = torch.zeros(len(embeddings), max_length, num_tokens, embed_dim)\n",
    "    mask = torch.zeros(len(embeddings), max_length)\n",
    "\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        padded_embeddings[i, : embedding.shape[0]] = embedding\n",
    "        mask[i, : embedding.shape[0]] = 1\n",
    "\n",
    "    return padded_embeddings, mask, torch.tensor(labels)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    "    collate_fn=collate_fn,\n",
    "    sampler=BalancedSampler(train_dataset),\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce1b82-1fd3-4005-a462-eb8d9a415c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 768\n",
    "combined_embed_dim = embed_dim * 4\n",
    "\n",
    "hidden_dim = 1024\n",
    "\n",
    "classifier_model = FullScanPredictor(combined_embed_dim, hidden_dim, num_labels=1).to(\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e9cf09-dd80-4c40-a2bb-f306c645112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epochs = 100\n",
    "\n",
    "accum_steps = 32\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    classifier_model.parameters(), momentum=0.9, weight_decay=0.01, lr=1e-3\n",
    ")\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_epochs, eta_min=1e-5)\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    classifier_model.eval()\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_iter = iter(val_loader)\n",
    "        for idx, (embeddings, _, labels) in enumerate(tqdm(val_iter)):\n",
    "            embeddings = embeddings.to(device)\n",
    "            logits = classifier_model(embeddings)\n",
    "\n",
    "            all_logits.append(logits.flatten().cpu().numpy())\n",
    "            all_labels.append(labels.float().cpu().numpy())\n",
    "\n",
    "    all_logits = np.array(all_logits)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    return compute_metrics(all_logits, all_labels)\n",
    "\n",
    "\n",
    "def train() -> int:\n",
    "\n",
    "    classifier_model.train()\n",
    "    losses = []\n",
    "    grad_norms = []\n",
    "    val_metrics = []\n",
    "\n",
    "    for train_epoch in range(train_epochs):\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_grad_norm = 0.0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_iter = iter(train_loader)\n",
    "\n",
    "        for idx, (embeddings, _, labels) in enumerate(tqdm(train_iter)):\n",
    "            logits = classifier_model(embeddings.to(device))\n",
    "            loss = criterion(logits.flatten(), labels.float().to(device))\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            total_grad_norm += torch.nn.utils.clip_grad_norm_(\n",
    "                classifier_model.parameters(), 1.0\n",
    "            ).item()\n",
    "\n",
    "            if (idx + 1) % accum_steps == 0 or idx == len(train_loader):\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        losses.append(total_loss / len(train_loader))\n",
    "        grad_norms.append(total_grad_norm / len(train_loader))\n",
    "\n",
    "        epoch_metrics = evaluate()\n",
    "        val_metrics.append(epoch_metrics)\n",
    "\n",
    "        accuracy = epoch_metrics[\"accuracy\"]\n",
    "        rocauc = epoch_metrics[\"aucroc\"]\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {train_epoch + 1}/{train_epochs} - Accuracy: {accuracy:.4f} - ROCAUC: {rocauc:.4f}\"\n",
    "        )\n",
    "\n",
    "    return losses, grad_norms, val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08f927d-6c51-49f3-828a-ecd5d706d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, grad_norms, val_metrics = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f358c2ff-a657-4d5b-ad65-f5cfdc36a428",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_metrics[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4ac140-8d33-4746-b156-f494a1681093",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dino2t24",
   "language": "python",
   "name": "dino2t24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

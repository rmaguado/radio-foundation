{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d98435-d274-4a4d-a36f-ec9b58e59411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad2ed65-9f9c-4a4b-9307-e6c93540a04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(file_path):\n",
    "    \"\"\"Load CSV file and ensure consistent formatting.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Convert all boolean columns to lowercase strings for consistency\n",
    "    for col in df.columns[1:]:  # Skip first column (image names)\n",
    "        df[col] = df[col].astype(str).str.lower().str.strip()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54cc155-21f9-4eb4-baa1-2fc5164710b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate precision, recall, f1, accuracy, and MCC.\"\"\"\n",
    "    # Convert to boolean numpy arrays\n",
    "    true_pos = ((y_true == \"true\") & (y_pred == \"true\")).sum()\n",
    "    false_pos = ((y_true == \"false\") & (y_pred == \"true\")).sum()\n",
    "    false_neg = ((y_true == \"true\") & (y_pred == \"false\")).sum()\n",
    "    true_neg = ((y_true == \"false\") & (y_pred == \"false\")).sum()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0\n",
    "    recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    accuracy = (true_pos + true_neg) / len(y_true) if len(y_true) > 0 else 0\n",
    "\n",
    "    # Calculate Matthews Correlation Coefficient (MCC)\n",
    "    # Denominator components\n",
    "    denom_tp_fp = true_pos + false_pos\n",
    "    denom_tp_fn = true_pos + false_neg\n",
    "    denom_tn_fp = true_neg + false_pos\n",
    "    denom_tn_fn = true_neg + false_neg\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy,\n",
    "        'support': len(y_true)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1473ae-24e0-49ad-9153-1a26cd131f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(ground_truth_path, model_paths):\n",
    "    \"\"\"\n",
    "    Compare model predictions against ground truth with full metrics.\n",
    "    \n",
    "    Args:\n",
    "        ground_truth_path: Path to ground truth CSV file\n",
    "        model_paths: Dictionary of {model_name: model_csv_path}\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing comparison results and missing image info\n",
    "    \"\"\"\n",
    "    # Load ground truth data\n",
    "    try:\n",
    "        gt_df = load_csv(ground_truth_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading ground truth: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize results storage\n",
    "    results = {\n",
    "        'tasks': list(gt_df.columns[1:]),  # Skip image name column\n",
    "        'model_metrics': defaultdict(dict),\n",
    "        'missing_images': defaultdict(list),\n",
    "        'image_counts': {}\n",
    "    }\n",
    "    \n",
    "    # Get all ground truth images\n",
    "    all_gt_images = set(gt_df.iloc[:, 0])\n",
    "    results['image_counts']['ground_truth'] = len(all_gt_images)\n",
    "    \n",
    "    # Compare each model\n",
    "    for model_name, model_path in model_paths.items():\n",
    "        try:\n",
    "            model_df = load_csv(model_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {model_name} data: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Get model images and report missing ones\n",
    "        model_images = set(model_df.iloc[:, 0])\n",
    "        missing_images = all_gt_images - model_images\n",
    "        results['missing_images'][model_name] = sorted(missing_images)\n",
    "        results['image_counts'][model_name] = len(model_images)\n",
    "        \n",
    "        # Use only images present in both ground truth and model\n",
    "        common_images = all_gt_images.intersection(model_images)\n",
    "        if not common_images:\n",
    "            print(f\"Warning: No common images between ground truth and {model_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Filter and sort dataframes by image name\n",
    "        gt_common = gt_df[gt_df.iloc[:, 0].isin(common_images)]\n",
    "        model_common = model_df[model_df.iloc[:, 0].isin(common_images)]\n",
    "        \n",
    "        # Sort both dataframes by image name to ensure alignment\n",
    "        gt_common = gt_common.sort_values(by=gt_df.columns[0]).reset_index(drop=True)\n",
    "        model_common = model_common.sort_values(by=model_df.columns[0]).reset_index(drop=True)\n",
    "        \n",
    "        # Verify the same tasks are present\n",
    "        if set(gt_common.columns[1:]) != set(model_common.columns[1:]):\n",
    "            print(f\"Warning: Model {model_name} has different tasks than ground truth\")\n",
    "            # Continue with intersection of tasks\n",
    "            common_tasks = set(gt_common.columns[1:]).intersection(set(model_common.columns[1:]))\n",
    "            gt_common = gt_common[gt_common.columns[:1].tolist() + list(common_tasks)]\n",
    "            model_common = model_common[model_common.columns[:1].tolist() + list(common_tasks)]\n",
    "            if 'tasks_updated' not in results:\n",
    "                results['tasks_updated'] = {}\n",
    "            results['tasks_updated'][model_name] = list(common_tasks)\n",
    "        \n",
    "        # Calculate metrics for each task\n",
    "        tasks_to_evaluate = results['tasks']\n",
    "        if 'tasks_updated' in results and model_name in results['tasks_updated']:\n",
    "            tasks_to_evaluate = results['tasks_updated'][model_name]\n",
    "        \n",
    "        for task in tasks_to_evaluate:\n",
    "            try:\n",
    "                metrics = calculate_metrics(gt_common[task].values, model_common[task].values)\n",
    "                results['model_metrics'][model_name][task] = metrics\n",
    "            except KeyError:\n",
    "                print(f\"Warning: Task {task} not found in model {model_name}\")\n",
    "                continue\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3943c326-953b-4f6c-8a16-07776efc88d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(results, output_file=None):\n",
    "    \"\"\"Generate a detailed comparison report with all metrics.\"\"\"\n",
    "    if not results:\n",
    "        return \"No results to report.\"\n",
    "    \n",
    "    report = []\n",
    "    report.append(\"Model Comparison Report\")\n",
    "    report.append(\"=\" * 40)\n",
    "    report.append(f\"Tasks evaluated: {', '.join(results['tasks'])}\")\n",
    "    \n",
    "    # Report image counts and missing images\n",
    "    report.append(\"\\nImage Counts:\")\n",
    "    report.append(\"-\" * 20)\n",
    "    report.append(f\"Ground truth images: {results['image_counts']['ground_truth']}\")\n",
    "    for model in results['image_counts']:\n",
    "        if model != 'ground_truth':\n",
    "            report.append(f\"{model}: {results['image_counts'][model]} (missing {len(results['missing_images'][model])})\")\n",
    "    \n",
    "    # Report metrics for each task\n",
    "    report.append(\"\\nDetailed Metrics by Task:\")\n",
    "    report.append(\"-\" * 20)\n",
    "    \n",
    "    for task in results['tasks']:\n",
    "        report.append(f\"\\nTask: {task}\")\n",
    "        report.append(\"-\" * len(f\"Task: {task}\"))\n",
    "        \n",
    "        # Get all models that have this task\n",
    "        task_data = []\n",
    "        for model in results['model_metrics']:\n",
    "            if task in results['model_metrics'][model]:\n",
    "                metrics = results['model_metrics'][model][task]\n",
    "                task_data.append((\n",
    "                    model,\n",
    "                    metrics['accuracy'],\n",
    "                    metrics['precision'],\n",
    "                    metrics['recall'],\n",
    "                    metrics['f1'],\n",
    "                    metrics['support']\n",
    "                ))\n",
    "        \n",
    "        # Sort by F1 score (descending)\n",
    "        task_data.sort(key=lambda x: x[4], reverse=True)\n",
    "        \n",
    "        # Add header\n",
    "        report.append(f\"{'Model':<15} {'Precision':<10} {'Recall':<10} {'F1':<10}\")\n",
    "        report.append(\"-\" * 75)\n",
    "        \n",
    "        # Add metrics for each model\n",
    "        for data in task_data:\n",
    "            model, acc, prec, rec, f1, supp = data\n",
    "            report.append(\n",
    "                f\"{model:<15} {prec:.04f}      {rec:.04f}      {f1:.04f}\"\n",
    "            )\n",
    "    \n",
    "    # Report missing images if any\n",
    "    has_missing = any(len(missing) > 0 for missing in results['missing_images'].values())\n",
    "    if has_missing:\n",
    "        report.append(\"\\nMissing Images by Model:\")\n",
    "        report.append(\"-\" * 20)\n",
    "        for model, missing in results['missing_images'].items():\n",
    "            if missing:\n",
    "                report.append(f\"\\n{model} missing {len(missing)} images:\")\n",
    "                # Display up to 5 missing images to avoid cluttering the report\n",
    "                display_count = min(5, len(missing))\n",
    "                report.append(\", \".join(missing[:display_count]))\n",
    "                if len(missing) > display_count:\n",
    "                    report.append(f\"... and {len(missing) - display_count} more\")\n",
    "    \n",
    "    full_report = \"\\n\".join(report)\n",
    "    \n",
    "    if output_file:\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(full_report)\n",
    "    \n",
    "    return full_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eca2063-6661-4ed6-b213-22c8ec74cab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(results, output_dir=\"figures\"):\n",
    "    \"\"\"\n",
    "    Generate academic report quality figures for each metric and task.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary containing comparison results.\n",
    "        output_dir: Directory to save the figures.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to plot.\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    metrics_to_plot = ['f1']\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    plot_data = defaultdict(lambda: defaultdict(dict))\n",
    "    for model_name, tasks_metrics in results['model_metrics'].items():\n",
    "        for task_name, metrics in tasks_metrics.items():\n",
    "            for metric_name, value in metrics.items():\n",
    "                if metric_name in metrics_to_plot:\n",
    "                    plot_data[metric_name][task_name][model_name] = value\n",
    "\n",
    "    for metric_name in metrics_to_plot:\n",
    "        df_metric = pd.DataFrame()\n",
    "        for task_name, model_values in plot_data[metric_name].items():\n",
    "            for model_name, value in model_values.items():\n",
    "                df_metric = pd.concat([df_metric, pd.DataFrame({\n",
    "                    'Model': [model_name],\n",
    "                    'Task': [task_name],\n",
    "                    'Value': [value]\n",
    "                })], ignore_index=True)\n",
    "        \n",
    "        task_order = df_metric.groupby('Task')['Value'].max().sort_values(ascending=False).index\n",
    "        \n",
    "        df_metric['Task'] = pd.Categorical(df_metric['Task'], categories=task_order, ordered=True)\n",
    "\n",
    "        sns.set_theme(style=\"whitegrid\", palette=\"viridis\")\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        ax = sns.barplot(data=df_metric, x='Task', y='Value', hue='Model', \n",
    "                         edgecolor=\".2\", linewidth=0.5)\n",
    "        \n",
    "        plt.xlabel('Abnormality', fontsize=14)\n",
    "        plt.ylabel(f'{metric_name.replace(\"_\", \" \").title()}', fontsize=14)\n",
    "        plt.ylim(0, 1.01) # Metrics are between 0 and 1\n",
    "        plt.xlim(-0.8, 14.5)\n",
    "        plt.xticks(rotation=45, ha='right', fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "\n",
    "        ax.yaxis.set_major_locator(MultipleLocator(0.2))\n",
    "        ax.yaxis.set_minor_locator(MultipleLocator(0.1))\n",
    "\n",
    "        n_models = len(results['model_metrics'])\n",
    "        \n",
    "        plt.legend(\n",
    "            loc='upper center', \n",
    "            bbox_to_anchor=(0.5, 1.15), \n",
    "            ncol=n_models,\n",
    "            frameon=False,\n",
    "            fontsize=14\n",
    "        )\n",
    "        plt.tight_layout(rect=[0, 0, 0.88, 1])\n",
    "\n",
    "        ax.grid(True, axis='y', alpha=0.7)\n",
    "        ax.grid(which='major', linestyle='-', linewidth='0.5', color='black')\n",
    "        ax.grid(which='minor', linestyle='-', linewidth='0.2', color='black')\n",
    "    \n",
    "        ax.grid(False, axis='x')\n",
    "        \n",
    "        figure_path = os.path.join(output_dir, f'{metric_name}_performance.png')\n",
    "        plt.savefig(figure_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Generated figure: {figure_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd369275-48f9-4031-a33b-e7d00c907ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_file = \"out/ground_truth/ct_findings_classification.csv\"\n",
    "model_files = {\n",
    "    \"DINO-B\": \"out/cls_frozen_onestep_gen/ct_findings_classification.csv\",\n",
    "    \"DINO-L\": \"out/cls_frozen_onestep_large_gen/ct_findings_classification.csv\",\n",
    "    \"DINO-B(CT-RATE)\": \"out/cls_frozen_onestep_ct_rate_gen/ct_findings_classification.csv\",\n",
    "    \"CT-CLIP\": \"out/ct_clip_onestep_gen/ct_findings_classification.csv\",\n",
    "    \"CT-FM\": \"out/ct_fm_onestep_gen/ct_findings_classification.csv\",\n",
    "}\n",
    "\n",
    "# Verify files exist\n",
    "if not os.path.exists(ground_truth_file):\n",
    "    print(f\"Error: Ground truth file {ground_truth_file} not found\")\n",
    "    exit()\n",
    "\n",
    "missing_models = [name for name, path in model_files.items() if not os.path.exists(path)]\n",
    "if missing_models:\n",
    "    print(f\"Error: Missing model files for {', '.join(missing_models)}\")\n",
    "    exit()\n",
    "\n",
    "# Compare models\n",
    "comparison_results = compare_models(ground_truth_file, model_files)\n",
    "\n",
    "# Generate and print report\n",
    "report = generate_report(comparison_results, output_file=\"model_comparison_report.txt\")\n",
    "#print(report)\n",
    "\n",
    "# Generate and save academic quality figures\n",
    "plot_metrics(comparison_results, output_dir=\"out/figures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2995fa95-e65a-4b6b-a634-faf31da14962",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = comparison_results['model_metrics']\n",
    "tasks = comparison_results['tasks']\n",
    "\n",
    "f1_only = {model: {task:m[\"f1\"] for task, m in tasks_f1.items()} for model, tasks_f1 in metrics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baba35c4-e9ef-4f0f-bf1e-c0486388b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, tasks_perf in f1_only.items():\n",
    "    print(model)\n",
    "    values = list(tasks_perf.values())\n",
    "    means = np.mean(values)\n",
    "    se = np.std(values) * 0.717\n",
    "    print(f\"{means:.03f}Â±{se:.03f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8ce3ef-bbf3-4d3f-af2c-8f7a478fc1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa7abb5-0ea8-4bb1-8c5c-d45005531f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_perf = {l:[] for l in tasks}\n",
    "\n",
    "model_names = [\"DINO-B\", \"DINO-L\", \"DINO-B(CT-RATE)\"] #[\"CT-CLIP\", \"CT-FM\"] # \n",
    "\n",
    "for model_name, tasks_performance in f1_only.items():\n",
    "    \n",
    "    if model_name not in model_names:\n",
    "        continue\n",
    "\n",
    "    print(model_name)\n",
    "    \n",
    "    for label in tasks:\n",
    "\n",
    "        label_perf[label].append(f\"{tasks_performance[label]:.03f}\")\n",
    "\n",
    "for label, row in label_perf.items():\n",
    "    \n",
    "    print(f\"{label} & \" + \" & \".join(row) + \"\\\\\\\\\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aab433-760e-4d6f-8761-fc90bb01292f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lighter_zoo import SegResEncoder\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    LoadImage,\n",
    "    EnsureType,\n",
    "    Orientation,\n",
    "    ScaleIntensityRange,\n",
    "    CropForeground,\n",
    "    Resize\n",
    ")\n",
    "from monai.inferers import SlidingWindowInferer\n",
    "\n",
    "from einops import rearrange\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"CT-RATE_train_eval\"\n",
    "dataset_split = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "project_path = os.getenv(\"PROJECTPATH\")\n",
    "run_name = \"CT-FM\"\n",
    "checkpoint_name = \"embeddings\"\n",
    "\n",
    "output_path = os.path.join(\n",
    "    project_path,\n",
    "    \"evaluation/cache\",\n",
    "    dataset_name,\n",
    "    run_name,\n",
    "    checkpoint_name,\n",
    ")\n",
    "os.makedirs(output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = SegResEncoder.from_pretrained(\"project-lighter/ct_fm_feature_extractor\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Loaded model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Compose(\n",
    "    [\n",
    "        LoadImage(ensure_channel_first=True),\n",
    "        EnsureType(),\n",
    "        Orientation(axcodes=\"SPL\"),\n",
    "        ScaleIntensityRange(\n",
    "            a_min=-1024,\n",
    "            a_max=2048,\n",
    "            b_min=0,\n",
    "            b_max=1,\n",
    "            clip=True,\n",
    "        ),\n",
    "        CropForeground(allow_smaller=True),\n",
    "        Resize(spatial_size=(240, 512, 512)),\n",
    "    ]\n",
    ")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.getenv(\"DATAPATH\")\n",
    "dataset_path = os.path.join(data_path, \"niftis/CT-RATE\", dataset_split)\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, dataset_path):\n",
    "        nifti_template = os.path.join(dataset_path, \"**/*.nii\")\n",
    "        self.nifti_paths = glob(nifti_template, recursive=True)\n",
    "    def __getitem__(self, index):\n",
    "        nifti_path = self.nifti_paths[index]\n",
    "        image_name = nifti_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        input_tensor = preprocess(nifti_path)\n",
    "        return input_tensor.unsqueeze(0), nifti_path\n",
    "    def __len__(self):\n",
    "        return len(self.nifti_paths)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    tensors, filenames = zip(*batch)\n",
    "\n",
    "    return tensors, filenames\n",
    "\n",
    "dataset = Dataset(dataset_path)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_tensors, image_names in tqdm(dataloader):\n",
    "\n",
    "    for i, image_name in enumerate(image_names):\n",
    "\n",
    "        input_tensor = input_tensors[i].to(device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)[-1].squeeze()\n",
    "            output = rearrange(output, \"e a w h -> a (w h) e\").cpu().numpy()\n",
    "        \n",
    "        np.save(os.path.join(output_path, f\"{image_name}.npy\"), output)\n",
    "\n",
    "        print(output.shape)\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctfm",
   "language": "python",
   "name": "ctfm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

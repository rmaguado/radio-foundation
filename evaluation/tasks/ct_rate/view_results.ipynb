{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from prettytable import PrettyTable\n",
    "import seaborn as sns\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "METRICS = [\"roc_auc\", \"f1\", \"precision\", \"recall\", \"specificity\", \"jw5\", \"jw6\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "project_path = os.getenv(\"PROJECTPATH\")\n",
    "data_path = os.getenv(\"DATAPATH\")\n",
    "\n",
    "experiment_name = \"ct_rate_cardiomegaly_patch\"\n",
    "label = \"Cardiomegaly\"\n",
    "results_path = os.path.join(project_path, \"runs\", experiment_name, \"results\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(result_path: str, valid_name: int, epoch: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Folds and epochs are 0-indexed.\n",
    "    \"\"\"\n",
    "    epoch_predictions_path = os.path.join(\n",
    "        result_path, valid_name, \"predictions\", f\"epoch_{epoch:02d}.csv\"\n",
    "    )\n",
    "    epoch_predictions = pd.read_csv(epoch_predictions_path)\n",
    "    logits = epoch_predictions[\"logits\"].values\n",
    "    labels = epoch_predictions[\"labels\"].values\n",
    "\n",
    "    return logits, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(logits: np.ndarray, labels: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate performance metrics based on logits and true labels.\n",
    "    \"\"\"\n",
    "    roc_auc = roc_auc_score(labels, logits)\n",
    "\n",
    "    probabilities = 1 / (1 + np.exp(-logits))\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    best_threshold = 0.5\n",
    "    best_youden = 0\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        predictions = (probabilities >= threshold).astype(int)\n",
    "\n",
    "        sensitivity = np.sum((predictions == 1) & (labels == 1)) / np.sum(labels == 1)\n",
    "        specificity = np.sum((predictions == 0) & (labels == 0)) / np.sum(labels == 0)\n",
    "        youden = sensitivity + specificity - 1\n",
    "\n",
    "        if youden > best_youden:\n",
    "            best_youden = youden\n",
    "            best_threshold = threshold\n",
    "\n",
    "    final_predictions = (probabilities >= best_threshold).astype(int)\n",
    "\n",
    "    final_precision = np.sum((final_predictions == 1) & (labels == 1)) / np.sum(final_predictions == 1)\n",
    "    final_recall = np.sum((final_predictions == 1) & (labels == 1)) / np.sum(labels == 1)\n",
    "    specificity = np.sum((final_predictions == 0) & (labels == 0)) / np.sum(labels == 0)\n",
    "    jw5 = (final_precision + final_recall) / 2\n",
    "    jw6 = final_precision*0.4 + final_recall*0.6\n",
    "    best_f1 = 2 * final_precision * final_recall / (final_precision + final_recall)\n",
    "\n",
    "    return {\n",
    "        \"roc_auc\": roc_auc, \n",
    "        \"f1\": best_f1,\n",
    "        \"precision\": final_precision,\n",
    "        \"recall\": final_recall,\n",
    "        \"specificity\": specificity,\n",
    "        \"jw5\": jw5,\n",
    "        \"jw6\": jw6,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_results(\n",
    "        results_path: str,\n",
    "        n_folds: int = 5,\n",
    "        n_epochs: int = 10\n",
    "    ) -> Tuple[Dict[str, List[float]], Dict[str, List[Tuple[float, float]]]]:\n",
    "    \"\"\"\n",
    "    Parse cross-validation results. Returns metrics for each epoch and fold, along with confidence intervals.\n",
    "\n",
    "    Args:\n",
    "        results_path: Path to results directory.\n",
    "        n_folds: Number of folds.\n",
    "        n_epochs: Number of epochs.\n",
    "\n",
    "    Returns:\n",
    "        metrics_epoch_fold: Dictionary of metrics for each epoch and fold. \n",
    "        confidence_intervals: Dictionary of mean and standard deviation for each metric and each epoch.\n",
    "    \"\"\"\n",
    "    epochs_performance = {m: {ep: [] for ep in range(n_epochs)} for m in METRICS}\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        for ep in range(n_epochs):\n",
    "            logits, labels = get_logits(results_path, f\"fold_{i}\", ep)\n",
    "\n",
    "            performance_metrics = get_metrics(logits, labels)\n",
    "            for metric_name, metric_value in performance_metrics.items():\n",
    "                epochs_performance[metric_name][ep].append(metric_value)\n",
    "\n",
    "    confidence_intervals = {\n",
    "        m: [(np.mean(perf), np.std(perf)) for perf in epochs_performance[m].values()]\n",
    "        for m in METRICS\n",
    "    }\n",
    "\n",
    "    return epochs_performance, confidence_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_epoch_from_metric(\n",
    "        epochs_performance: Dict[str, List[float]],\n",
    "        selection_metric: str = \"roc_auc\"\n",
    "    ) -> int:\n",
    "    \"\"\"\n",
    "    Select the best epoch based on the specified metric.\n",
    "    \"\"\"\n",
    "    epoch_mean_selection = [\n",
    "        np.mean(perf) for perf in epochs_performance[selection_metric].values()\n",
    "    ]\n",
    "\n",
    "    best_epoch = np.argmax(epoch_mean_selection)\n",
    "\n",
    "    return int(best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simple_folds_plot(\n",
    "        epochs_performance: Dict[str, List[float]],\n",
    "        label:str,\n",
    "        metric:str,\n",
    "        num_folds=5,\n",
    "        num_epochs=10,\n",
    "        cm=plt.cm.tab10\n",
    "    ) -> None:\n",
    "    plt.figure()\n",
    "\n",
    "    for fold_idx in range(num_folds):\n",
    "        epoch_metrics = [epochs_performance[metric][x][fold_idx] for x in range(num_epochs)]\n",
    "        plt.plot(epoch_metrics, color=cm(fold_idx), label=f\"Fold {fold_idx + 1}\")\n",
    "\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    plt.title(f\"Training Curve {metric} for {label}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ci_plot(\n",
    "        confidence_intervals:Dict[str, List[Tuple[float, float]]],\n",
    "        label:str,\n",
    "        metric:str,\n",
    "        num_epochs=10,\n",
    "        z_score=1.96,\n",
    "        ci_label:str = \"95% CI\",\n",
    "        full_y:bool = False\n",
    "    ) -> None:\n",
    "    plt.figure()\n",
    "\n",
    "    means = [ci[0] for ci in confidence_intervals[metric]]\n",
    "    stds = [ci[1] for ci in confidence_intervals[metric]]\n",
    "    ci = [std * z_score for std in stds]\n",
    "    \n",
    "    epochs = range(num_epochs)\n",
    "    plt.plot(epochs, means, label=f\"Mean {metric}\")\n",
    "    if full_y:\n",
    "        plt.ylim([0, 1])\n",
    "    plt.xticks(epochs)\n",
    "    plt.fill_between(epochs, np.array(means) - np.array(ci), np.array(means) + np.array(ci), alpha=0.2, label=ci_label)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(f\"Confidence Interval for {metric} - {label}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_val_epoch_plot(results_path, valid_name, epoch):\n",
    "\n",
    "    logits, labels = get_logits(results_path, valid_name, epoch)\n",
    "\n",
    "    prob_pos = logits[labels==1]\n",
    "    prob_neg = logits[labels==0]\n",
    "    \n",
    "    sns.kdeplot(prob_pos, label=\"Positive\", fill=True, alpha=0.5)\n",
    "    sns.kdeplot(prob_neg, label=\"Negative\", fill=True, alpha=0.5)\n",
    "\n",
    "    plt.xlabel(\"Predicted Probability\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.title(\"Density of Predicted Probabilities by Class\")\n",
    "    #plt.xlim((0, 1))\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_val_epoch_plot(results_path, \"test\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_performance, confidence_intervals = get_cv_results(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch_by_metric = {m:best_epoch_from_metric(epochs_performance, selection_metric=m) for m in METRICS} \n",
    "\n",
    "table = PrettyTable([\"Metric\", \"Best Epoch\"])\n",
    "\n",
    "for key, value in best_epoch_by_metric.items():\n",
    "    table.add_row([key, value])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_simple_folds_plot(epochs_performance, label, \"roc_auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_simple_folds_plot(epochs_performance, label, \"jw5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_ci_plot(confidence_intervals, label, \"roc_auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dino2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

#!/bin/bash

## NAME GPUS GPUTYPE WORKERS

#SBATCH --job-name=$NAME
#SBATCH --output=$OUT/$NAME/%j.out
#SBATCH --nodes=1
#SBATCH --nodelist=$NODE
#SBATCH --cpus-per-task=$WORKERS

#SBATCH --ntasks-per-node=$GPUS
#SBATCH --gres=gpu:$GPUS

#SBATCH --mem=96GB
#SBATCH --partition=odap-gpu
#SBATCH --signal=USR2@120
#SBATCH --time=14400

source $HOME/.conda_rc
conda activate mllm
cd $HOME/projects/radio-foundation
export PYTHONPATH=$PWD

export LOGDIR=$OUT/$NAME/logs
mkdir -p $LOGDIR

which python
pip list | grep xformers

deepspeed --enable_each_rank_log $LOGDIR mllm/llava/train/train.py \
    --deepspeed "mllm/configs/deepspeed/zero3.json" \
    --config_path $CONFIG \
    
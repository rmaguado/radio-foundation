model:
  model_name_or_path: "meta-llama/Llama-3.1-8B-Instruct"
  lora_backbone:
    lora_enable: False
    lora_r: 8
    lora_alpha: 16
    lora_dropout: 0.05
    lora_bias: "none"
    include_modules:
      - "qkv"
  lora_language:
    lora_enable: False
    lora_r: 128
    lora_alpha: 256
    lora_dropout: 0.05
    lora_bias: "none"
    include_modules:
      - "down_proj"
      - "up_proj"
      - "q_proj"
      - "gate_proj"
      - "q_proj"
      - "k_proj"
      - "v_proj"
  vision_tower: null
  mm_vision_select_layer: -1
  checkpoint_path: null
  mm_projector_type: "attn_pool"
  mm_vision_select_feature: "cls_patch"
  mm_vision_config_path: null
  mm_vision_checkpoint_path: null
data:
  root_path: null
  db_name: "CT-RATE_train_reports"
  channels: 10
  data_mean: 0.0
  data_std: 1.0
  is_multimodal: True
  image_tokens: 128
  conv_template: "plain"
train:
  cache_dir: "mllm/cache"
  optim: "adamw_torch"
  bits: 16
  learning_rate: 1e-3
  mm_projector_lr: 1e-4
  group_by_modality_length: False
  report_to: "none"
  num_train_epochs: 1
  per_device_train_batch_size: 32
  gradient_accumulation_steps: 1
  eval_strategy: "no"
  save_strategy: "steps"
  save_steps: 1
  save_total_limit: null
  weight_decay: 0.0
  warmup_ratio: 0.03
  lr_scheduler_type: "cosine"
  logging_steps: 1
  bf16: True
  tf32: True
  model_max_length: 1024
  gradient_checkpointing: True
  dataloader_num_workers: 4